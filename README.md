# AGV-ML-Learning


Logistic Regression has the Sigmoid function Associate every values with weights Initialise beta to random values at the start Yi would always either be 1 or 0

If training instance is 1, the RHS is irrelevant

Gradient ascent: Definition of 'goodness'

Finds max
Points into direction of increase and solution
For each of the beta values, imagine them to have values
Gradient descent: Definition of errors

Points into the direction of increase
Points away from the solution
Choose error function once that
